{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "##corpora-set of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\KIIT\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=brown.sents(categories=\"adventure\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dan',\n",
       " 'Morgan',\n",
       " 'told',\n",
       " 'himself',\n",
       " 'he',\n",
       " 'would',\n",
       " 'forget',\n",
       " 'Ann',\n",
       " 'Turner',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]\n",
    "#the first sentence that is present in the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data[0])\n",
    "#as you can see we have joined the letters that were present inside the corpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    Bags of word pipeline\n",
    "    the work of this pipeline is to covert the text data into the numeric data.we can use other classfication algo also to classify the data but we use the nlp because of the bag of words pipeline\n",
    "    \n",
    "    Text->numbers->classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bags of word pipeline:\n",
    "    .Get the data/corpus\n",
    "    .Tokenisation,Stopword removal\n",
    "    .Stemming/Lemmetization\n",
    "    .Building the vocab\n",
    "    .Vectorization\n",
    "    .Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document=\"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to market to buy some fruits\"\"\"\n",
    "sentence=\"Send all the 50 documents related to chapters 1,2,3 to shivam@gmail.com\"\n",
    "text = \"God is Great! I won a lottery.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a very pleasant day.',\n",
       " 'The weather was cool and there were light showers.',\n",
       " 'I went to market to buy some fruits']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Tokenisation/Stopword Removal\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "sent=sent_tokenize(document)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a very pleasant day.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'to', 'shivam', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "word=word_tokenize(sentence)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Send'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw=set(stopwords.words('english') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'now', 'has', 'herself', \"isn't\", 'few', 're', \"it's\", 'than', 'too', 'wouldn', 'isn', \"you'll\", 'when', 'about', 'my', 'or', 'he', 'she', 'some', 'himself', 'ourselves', 'didn', 'be', 'nor', 't', 'your', 'its', 'between', 'mustn', \"mustn't\", 'at', 'before', 'after', 'if', 'y', 'i', \"you'd\", 'are', 'had', \"haven't\", 'couldn', 'our', 'weren', 'can', 'hadn', 'for', \"weren't\", 'theirs', 'over', 'a', 'against', 'just', 'during', 'the', 'have', 'only', 'own', 'yours', 'which', 'them', 'but', 'then', \"wasn't\", 'all', \"she's\", 'themselves', 'we', 'am', 'having', 'by', 'him', \"wouldn't\", 'who', 'hasn', 'that', 'don', 'does', 'out', 'of', \"you've\", 'o', 'most', 've', 'while', 'ours', 'his', \"needn't\", 'myself', 'an', 'do', 'me', \"shouldn't\", 'in', 'down', \"didn't\", 'because', 'as', 'on', 'should', 'itself', 'it', 'doing', 'through', \"don't\", 'shan', 'again', 'same', 'those', 'is', \"won't\", 'won', 'ain', 'this', 'were', 'yourselves', 'other', 'not', 'where', 'd', 'ma', 'until', 'why', 'll', \"shan't\", \"you're\", 'to', 'wasn', 'above', 'off', 'hers', 'so', 'very', \"should've\", 'into', \"mightn't\", 'shouldn', 'aren', 'more', 'further', 'they', 'their', \"doesn't\", 'haven', 'mightn', 'did', 'each', 'you', 'such', 'these', 's', 'what', 'once', 'from', \"aren't\", 'been', 'no', 'her', 'any', 'doesn', 'was', \"hasn't\", 'both', \"that'll\", 'below', 'will', 'm', 'yourself', 'under', \"couldn't\", \"hadn't\", 'needn', 'with', 'here', 'up', 'how', 'there', 'being', 'and', 'whom'}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    useful_words=[w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text=\"I am very intelligent and you are dumb\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'intelligent', 'dumb']\n"
     ]
    }
   ],
   "source": [
    "useful_text=remove_stopwords(text,sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence=\"send all the 50 documents related to chapter 1,2,3 to shivam@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-z]+')\n",
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapter',\n",
       " 'to',\n",
       " 'shivam',\n",
       " 'gmail',\n",
       " 'com']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer('[a-zA-z@]+')\n",
    "useful_text=tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapter',\n",
       " 'to',\n",
       " 'shivam@gmail',\n",
       " 'com']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#there are three types of the stemmer:snowball stemmer,porter stemmer,lancaster stemmer\n",
    "#the special thing about the snowball stemmer is that it is multilanguage stemmer it also supports some of the language like french,german etc\n",
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn=SnowballStemmer('english') #in the snowball stemmer it is nescssary to give the arguement if we dont give the arguement then the snowball stemmer will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump-'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn.stem('jump-ing')\n",
    "#in this case it is not maching with the any word so it is the basic rule that if at the end there is ing then the ing will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SAMPLE CORPUS-CONTAINS 4 DOCUMEENTS AND EACH IF THE DOCUMENT HAS ONE OR MORE THAN ONE SENTENCES\n",
    "corpus=['Indian cricket team will win the world cup says the capt.Virat kohli.World cup will held in Srilanka.',\n",
    "       'We will win the next loksabha election says the confident Indian PM.',\n",
    "       'The nobel laurate won the hearts of people.',\n",
    "       'The movie raazi is an exciting Indian spy thriller based upon the real story.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the vectors tell us what word is appearing how many number of times\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_corpus=cv.fit_transform(corpus)\n",
    "#the fit_transform will convert it into the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x38 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus\n",
    "#as you can see it is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 37)\t2\n",
      "  (0, 29)\t2\n",
      "  (0, 35)\t1\n",
      "  (0, 34)\t2\n",
      "  (0, 28)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 29)\t2\n",
      "  (1, 35)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 29)\t2\n",
      "  (3, 27)\t1\n",
      "  (3, 23)\t1\n",
      "  (3, 31)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 25)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 29)\t2\n",
      "  (3, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus )\n",
    "#it means in the zeroth sentence the first word is getting the index of 11 in the dictionary and value at that index is 1\n",
    "#simpilarly we can see for the other words in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 2 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 2 0 0 1 0 2 1 0\n",
      "  2]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 2 0 0 0 1 1 1 0\n",
      "  0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1\n",
      "  0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 2 1 1 0 0 0 0 0\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0,\n",
       " 'based': 1,\n",
       " 'capt': 2,\n",
       " 'confident': 3,\n",
       " 'cricket': 4,\n",
       " 'cup': 5,\n",
       " 'election': 6,\n",
       " 'exciting': 7,\n",
       " 'hearts': 8,\n",
       " 'held': 9,\n",
       " 'in': 10,\n",
       " 'indian': 11,\n",
       " 'is': 12,\n",
       " 'kohli': 13,\n",
       " 'laurate': 14,\n",
       " 'loksabha': 15,\n",
       " 'movie': 16,\n",
       " 'next': 17,\n",
       " 'nobel': 18,\n",
       " 'of': 19,\n",
       " 'people': 20,\n",
       " 'pm': 21,\n",
       " 'raazi': 22,\n",
       " 'real': 23,\n",
       " 'says': 24,\n",
       " 'spy': 25,\n",
       " 'srilanka': 26,\n",
       " 'story': 27,\n",
       " 'team': 28,\n",
       " 'the': 29,\n",
       " 'thriller': 30,\n",
       " 'upon': 31,\n",
       " 'virat': 32,\n",
       " 'we': 33,\n",
       " 'will': 34,\n",
       " 'win': 35,\n",
       " 'won': 36,\n",
       " 'world': 37}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "#which word is getting which index in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizer(document):\n",
    "    words=tokenizer.tokenize(document.lower())\n",
    "    #remove stopwords\n",
    "    words=remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuction']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytokenizer('this is some fuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv=CountVectorizer(tokenizer=mytokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_corpus=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##test data\n",
    "test_corpus=[\"Indian are very great\"]\n",
    "cv.transform(test_corpus).toarray()\n",
    "#if you fit_transform while using the test data then it will create the problem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  MORE WAYS TO CREATE FEATURES:\n",
    "    -UNIGRAM:EVERY WORD AS A FEATURE\n",
    "    -BIGRAMS\n",
    "    -TRIGRAMS\n",
    "    -N-GRAMS\n",
    "    -TF-IDF NORMALISATION\n",
    " #basically what is there that above we are treating each word as the single feature.so what we do in this we club two or more words to create the single feature.so there are some advantages of clubbing the two aords and there are also some disadvantages.\n",
    " the disadvantage is that it makes difficult to for the computer to analyze the sense of the sentence for example in the case of the negation the computer is not able to differentiate between the not good and good as we have clubbed the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_1=['this is good movie']\n",
    "sent_2=['this is good movie but actor is not present']\n",
    "sent_3=['this is not good movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,3))\n",
    "docs=[sent_1[0],sent_2[0]]\n",
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF NORMALISATION:\n",
    ".AVOID FEATURES THAT OCCUR VERY OFTEN BECAUSE THEY CONTAIN THE VERY LESS INFORMATION fort example the word 'the' etc\n",
    ".INFORMATION DECREASE AS THE NUMBER OF THE OCCURENCES INCREASE ACROSS DIFFERENT TYPE OF DOCUMENT\n",
    ".SO WE DEFINE THE ANOTHER TERM-TERM DOCUMENT FREQUENCY WHICH ASSOCAITES A WEIGHT WITH EVERY\n",
    "#tf-idf is basically the product of the two terms first term is the term frequency denoted by tf(t,d) it means the term t and the document d and the second term is the inverse document frquecy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_1='this is a good movie'\n",
    "sent_2='this was good movie'\n",
    "sent_3='this is not good movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vc=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46333427  0.59662724  0.46333427  0.          0.46333427  0.        ]\n",
      " [ 0.41285857  0.          0.41285857  0.          0.41285857  0.69903033]\n",
      " [ 0.3645444   0.46941728  0.3645444   0.61722732  0.3645444   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'is': 1, 'movie': 2, 'not': 3, 'this': 4, 'was': 5}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_\n",
    "#we can see from the above the word that is occuring very less has very low weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hey', '.', 'wonder', '!', 'Welcome', 'worlds', 'to', 'the', 'of', '?'}\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sent = \"Hey! Welcome to the worlds of wonder?.\"\n",
    "words = set(word_tokenize(sent))\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
